{% from "ceph/map.jinja" import ceph_settings with context %}

[global]
    fsid = {{ ceph_settings.fsid }}
    public network = {{ salt['pillar.get']('ceph:global:public_network') }}
    cluster network = {{ salt['pillar.get']('ceph:global:cluster_network') }}
    auth cluster required = cephx
    auth service required = cephx
    auth client required = cephx

[osd]
    osd journal size = {{ salt['pillar.get']('ceph:osd:journal_size') }}
    osd pool default size = {{ salt['pillar.get']('ceph:osd:pool_default_size') }}
    osd pool default min size = {{ salt['pillar.get']('ceph:osd:pool_default_min_size') }}
    osd pool default pg num = {{ salt['pillar.get']('ceph:osd:pool_default_pg_num') }}
    osd pool default pgp num = {{ salt['pillar.get']('ceph:osd:pool_default_pgp_num') }}
    osd crush chooseleaf type = {{ salt['pillar.get']('ceph:osd:crush_chooseleaf_type') }}
    osd crush update on start = {{ salt['pillar.get']('ceph:osd:crush_update_on_start') }}
    filestore merge threshold = {{ salt['pillar.get']('ceph:osd:filestore_merge_threshold') }}
    filestore split multiple = {{ salt['pillar.get']('ceph:osd:filestore_split_multiple') }}
    filestore op threads = {{ salt['pillar.get']('ceph:osd:filestore_op_threads') }}
    filestore max sync interval = {{ salt['pillar.get']('ceph:osd:filestore_max_sync_interval') }}
    filestore min sync interval = {{ salt['pillar.get']('ceph:osd:filestore_min_sync_interval') }}
    filestore queue max ops = {{ salt['pillar.get']('ceph:osd:filestore_queue_max_ops') }}
    filestore queue max bytes = {{ salt['pillar.get']('ceph:osd:filestore_queue_max_bytes') }}
    filestore queue committing max ops = {{ salt['pillar.get']('ceph:osd:filestore_queue_committing_max_ops') }}
    filestore queue committing max bytes = {{ salt['pillar.get']('ceph:osd:filestore_queue_committing_max_bytes') }}
    osd op threads = {{ salt['pillar.get']('ceph:osd:op_threads') }}
    osd disk threads = {{ salt['pillar.get']('ceph:osd:disk_threads') }}
    osd max backfills = {{ salt['pillar.get']('ceph:osd:max_backfills') }}
    osd map cache size = {{ salt['pillar.get']('ceph:osd:map_cache_size') }}
    osd scrub load threshold = {{ salt['pillar.get']('ceph:osd:scrub_load_threshold') }}

[client]
    rbd cache = {{ salt['pillar.get']('ceph:client:rbd_cache') }}
    rbd cache writethrough until flush = {{ salt['pillar.get']('ceph:client:rbd_cache_writethrough_until_flush') }}
    rbd cache size = {{ salt['pillar.get']('ceph:client:rbd_cache_size') }}


{% for mon, ip_map in salt['mine.get']('roles:ceph-mon','ip_map','grain').items() -%}
{% set tmp, mon_hostname = salt['mine.get'](mon, 'hostname').items()[0] %}

[mon.{{ mon_hostname }}]
    host = {{ mon_hostname }}
    mon addr = {{ ip_map[ceph_settings.mon_interface][0] }}:6789

{% endfor -%}

{% for rgw, ip_map in salt['mine.get']('roles:ceph-rgw','ip_map','grain').items() -%}
{% set tmp, rgw_hostname = salt['mine.get'](rgw, 'hostname').items()[0] %}

[client.radosgw.gateway]
    host = {{ rgw_hostname }}
    keyring = {{ ceph_settings.radosgw_keyring }}
    rgw frontends = "civetweb port=80"
    log file = /var/log/radosgw/client.radosgw.gateway.log

{% endfor -%}

